import selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.keys import Keys
from nltk.tokenize import sent_tokenize, word_tokenize
import nltk
import csv
import requests
from bs4 import BeautifulSoup



driver = webdriver.Chrome(executable_path= 'C:/Users/rajme/Downloads/chromedriver_win32/chromedriver')

driver.get("https://stackoverflow.com/")
# Wait 20 seconds for page to load
timeout = 20
try:
    WebDriverWait(driver, timeout)
except TimeoutException:
    print("Timed out waiting for page to load")
    driver.quit()

topic = "artificial intelligence"
elem = driver.find_element_by_name("q")
elem.clear()
elem.send_keys(topic)
elem.send_keys(Keys.RETURN)
driver.find_element_by_xpath('//*[@title="show 50 items per page"]').click()

links=[]
list_links = driver.find_elements_by_tag_name('a')
for i in list_links:
    links.append(i.get_attribute('href'))

linksf=[]
for i in range(0,len(links)):
    if type(links[i]) == str:
        linksf.append(links[i])


ml=[]
for i in range (0,len(linksf)):
        tokens1=linksf[i].split("/")
 #       print(ml[i])
  #      print(tokens1)
        for j in tokens1:
      #      print(j)
            if j == "questions":
        #        print("no")
                ml.append(linksf[i])


no_ques=[]
for i in range (0,len(ml)):
        tokens1=ml[i].split("/")
 #       print(ml[i])
  #      print(tokens1)
        for j in tokens1:
    #        print(j)
            if j != "tagged" and j == "stackoverflow.com":
  #              print("no")
                no_ques.append(ml[i]) # I want

no_ques2=[]
for i in range (0,len(no_ques)):
        tokens1=no_ques[i].split("/")
 #       print(ml[i])
  #      print(tokens1)
        for j in tokens1:
 #           print(j)
            if j == "tagged":
 #               print("no")
                no_ques2.append(no_ques[i])

final_url  = [x for x in no_ques if x not in no_ques2]

no_ques3_final=[]
for i in range (0,len(final_url)):
        tokens1=final_url[i].split("/")
        if len(tokens1) > 5:
            no_ques3_final.append(final_url[i])


dict_list=[]
for i in range (0,len(no_ques3_final)):
    print(i)
    c=no_ques3_final[i]
    site = requests.get(c);
    if site.status_code is 200:
        soup = BeautifulSoup(site.content, 'html.parser')

        topic = soup.find(class_='question-hyperlink').get_text(strip=True)
        print(topic)
        description = soup.find(class_='post-text').get_text(strip=True)
        url =   soup.find(class_='question-hyperlink').get('href').strip()


        views = soup.find(class_='module question-stats').findAll('b')[1].get_text(strip=True)

        votes = soup.find(class_='vote-count-post ').get_text(strip=True)

        try:
            best_answer = soup.find("div", class_="answercell post-layout--right").find("div", class_="post-text").get_text(strip=True)
        except:
            pass
        updated_data = {"topic": topic,"description":description, "url": url, "views": views, "answers":best_answer, "votes":votes}
        dict_list.append(updated_data)

with open ("artificial intelligence.csv",'w') as file:
    writer = csv.DictWriter(file, fieldnames = ["topic","description", "url", "views", "answers", "votes"], delimiter = ',')
    writer.writeheader()
    for row in dict_list:
        writer.writerow(row)
